# Tutorial: Your first analysis with `dataframe` (Iris)

> A hands‑on, copy‑paste‑friendly tour of the **dataframe** ecosystem.

By the end of this tutorial you will have:

- loaded `iris.parquet` into a `DataFrame`
- inspected columns and rows
- filtered to a subset of rows
- derived a few new columns using typed expressions (`Expr a`)
- grouped + aggregated columns
- made a quick plot
- written results back out to CSV

**Assumptions**

- You have a file at `./data/iris.parquet`
- You’re using Cabal (if you prefer to use Jupyter notebooks (via IHaskell) see the “try it in a notebook” section at the end).

---

## 1) Create a tiny project you can run

In an empty folder:

```bash
mkdir iris
cd iris
cabal init -n --exe --minimal
```

Add `dataframe` and `text` (the recommended Haskell string library) to your `.cabal` file under `build-depends` for the executable:

```
build-depends: base, dataframe, text
```

**Checkpoint**: Run the program with `cabal run`. You should see `Hello, Haskell` shown in the terminal.

---

## 2) Load Iris and verify the shape

We’ll do two things:

* Generate typed column expressions from the CSV header (Template Haskell)
* Read the Parquet file into a DataFrame

Add this near the top of Main.hs (above main):

```haskell
-- We need this extenstion to generate
-- column references for our dataframe.
{-# LANGUAGE TemplateHaskell #-}

module Main where

import qualified DataFrame as D
import qualified DataFrame.Functions as F

-- We default to reading strings as `Text`
-- So you need this import here to make the
-- column generation work.
import Data.Text (Text)

-- Reads the file, determines what the types of the column
-- are and creates references to the columns.
$(F.declareColumnsFromParquetFile "../dataframe/data/iris.parquet")

main :: IO ()
main = putStrLn "Hello, Haskell!"
```

You should see the names of the generated symbols when the code compiles.

```haskell
$ cabal run
Resolving dependencies...
Build profile: -w ghc-9.8.4 -O1
In order, the following will be built (use -v for more details):
 - iris-0.1.0.0 (exe:iris) (configuration changed)
Configuring executable 'iris' for iris-0.1.0.0...
Preprocessing executable 'iris' for iris-0.1.0.0...
Building executable 'iris' for iris-0.1.0.0...
[1 of 1] Compiling Main             ( app/Main.hs, /home/user/code/iris/dist-newstyle/build/x86_64-linux/ghc-9.8.4/iris-0.1.0.0/x/iris/build/iris/iris-tmp/Main.o ) [Source file changed]
sepal_length :: Expr Double
sepal_width :: Expr Double
petal_length :: Expr Double
petal_width :: Expr Double
variety :: Expr Text
```

Now we can change `main` to read the file and print the information we care about.

```haskell
main :: IO ()
main = do
  df <- D.readParquet ("/data/iris.parquet")
  print (D.dimensions df)
  print (D.take 5 df)
```

## 3) Select the columns you care about
In early exploration, selecting a few columns makes everything easier to read.

Let's, for some time, just look at the petal columns.

```haskell
main :: IO ()
main = do
  df <- D.readParquet "../dataframe/data/iris.parquet"
  print (D.select [F.name petal_length, F.name petal_width, F.name variety] df)
```

In the code above we retrieve the underlying column names from the column references generated by `declareColumnsFromParquetFile`. This means we can use tab-completions for column names. Additionally,
it means we can avoid typos!

## 4) Filter rows with typed predicates

Filtering is where the `Expr`s start to pay off: you can build type-safe predicates.

```haskell
-- We need this extenstion to generate
-- column references for our dataframe.
{-# LANGUAGE TemplateHaskell #-}
{-# LANGUAGE OverloadedStrings #-}

module Main where

import qualified DataFrame as D
import qualified DataFrame.Functions as F

-- Some comparison functions for expressions.
-- We export them separately so we can use them
-- without type `F.`.
import DataFrame.Functions ((.==), (.>), (.<), (.&&))

import Data.Text (Text)

$(F.declareColumnsFromParquetFile "./data/iris.parquet")

main :: IO ()
main = do
  df <- D.readParquet "./data/iris.parquet"

  -- String literals are automatically "promoted" to expressions.
  let dfV = D.filterWhere (variety .== "Setosa")  df
  -- So are number literals.
  let dfC = D.filterWhere (petal_length .> 4.5 .&& (petal_width .< 1.3)) df

  print (D.dimensions dfV)
  print (D.take 5 dfC)
```

## 5) Derive new columns (your first "feature engineering" step)

You’ll often want new columns that are:

* arithmetic combinations (ratio, area),
* boolean flags, or,
* normalized features (zScore)

Again, our type-safe column references make this code safer and easier to write.

```haskell
{-# LANGUAGE TemplateHaskell #-}
{-# LANGUAGE OverloadedStrings #-}

module Main where

import qualified DataFrame as D
import qualified DataFrame.Functions as F

-- include import for `as`.
import DataFrame.Functions ((.==), (.>), (.<), (.&&), as)

-- import chaining operator
import DataFrame ((|>))
import Data.Text (Text)

$(F.declareColumnsFromParquetFile "../dataframe/data/iris.parquet")

main :: IO ()
main = do
  df <- D.readParquet "../dataframe/data/iris.parquet"
  let df2 =
        df
          |> D.deriveMany
              [ (sepal_length / sepal_width) `as` "sepal_ratio"
              , (petal_width * petal_length) `as` "petal_area"
              , (sepal_width .> 3.0)         `as` "has_wide_sepal"
              , (F.zScore petal_length)      `as` "zscore_petal_length"
              , (F.pow petal_width 2)        `as` "petal_width_squared"
              ]

  print (D.take 5 df2)
```

Alternatively we can also use the `.=` operator to define columns.

```haskell
-- We need this extenstion to generate
-- column references for our dataframe.
{-# LANGUAGE TemplateHaskell #-}
{-# LANGUAGE OverloadedStrings #-}

module Main where

import qualified DataFrame as D
import qualified DataFrame.Functions as F

-- include import for `as`.
import DataFrame.Functions ((.==), (.>), (.<), (.&&), (.=))

-- import chaining operator
import DataFrame ((|>))

-- We default to reading strings as `Text`
-- So you need this import here to make the
-- column generation work.
import Data.Text (Text)

-- Reads the file, determines what the types of the column
-- are and creates references to the columns.
$(F.declareColumnsFromParquetFile "../dataframe/data/iris.parquet")

main :: IO ()
main = do
  df <- D.readParquet "../dataframe/data/iris.parquet"
  let df2 =
        df
          |> D.deriveMany
              [ "sepal_ratio"         .= sepal_length / sepal_width
              , "petal_area"          .= petal_width * petal_length
              , "has_wide_sepal"      .= sepal_width .> 3.0
              , "zcore_petal_length"  .= F.zScore petal_length
              , "petal_width_squared" .= F.pow petal_width 2
              ]

  print (D.take 5 df2)
```

## 6) User defined functions

You can also use custom haskell functions to manipulate dataframe columns.
Say you had the following Haskell function that takes in the petal length and bucketized it.

```haskell
-- Custom dataframe types need to derive these 4 instances.
data PetalSize = SMALL | MEDIUM | LARGE deriving (Show, Ord, Eq, Read)
petalSize :: Double -> PetalSize
petalSize p
  | p >= 5.1  = LARGE
  | p >= 1.6  = MEDIUM
  | otherwise = SMALL
```

We can apply it to the `petal_length` column using the `lift` function.

```haskell
main :: IO ()
main = do
  df <- D.readParquet "../dataframe/data/iris.parquet"
  let df2 =
        df
          |> D.deriveMany
              [ "sepal_ratio"         .= sepal_length / sepal_width
              , "petal_area"          .= petal_width * petal_length
              , "has_wide_sepal"      .= sepal_width .> 3.0
              , "zcore_petal_length"  .= F.zScore petal_length
              -- Add petal size.
              , "petal_size"          .= F.lift petalSize petal_length
              , "petal_width_squared" .= F.pow petal_width 2
              ]

  print (D.take 5 df2)
```

If we wanted to apply a function that takes in two variables to our columns we would use the `lift2` function. For example, we can define `petal_area` as `"petal_area" .= F.lift2 (*) petal_width petal_length`.

## 7) Group + aggregate (summary stats per species)

Let's create a "report" that compute counts and basic stats per group.

```haskell
main :: IO ()
main = do
  df <- D.readParquet "../dataframe/data/iris.parquet"

  -- we've skipped the feature derivation code.

  let groups = D.groupBy ["variety"] df
  let stats =
        groups
          |> D.aggregate
              [ "n"      .= F.count petal_length
              , "meanPL" .= F.mean petal_length
              , "sdPL"   .= F.stddev petal_length
              , "minPW"  .= F.minimum petal_width
              , "maxPW"  .= F.maximum petal_width
              ]
  print stats
```

## 8) A simple scatter plot

We can show the sepal width and length by variety with the following terminal plot:

```haskell
D.plotScatterBy (F.name sepal_length) (F.name sepal_width) (F.name variety) df
```

dataframe's terminal plotting allows for quick and easy data exploration without having to leave the context of your terminal.

## 9) Save outputs you can share or reuse

Add the following to the end of your main function:

```haskell
D.writeCsv "out/iris_enriched.csv" df2
D.writeCsv "out/iris_stats.csv" stats
```

Run and check that the two CSV files are successfully generated.

```bash
mkdir -p out
cabal run
```

## IHaskell
You can open an iHaskell instance in binder or in our playground (instructions in README).
This environment comes with the imports and extensions preloaded so you can start coding
without boilerplate.


```haskell
-- Cell 1
df <- D.readParquet "./data/iris.parquet"
F.declareColumns df -- in notebooks we can run the column generation function at any point

-- Cell 2
df2 =
    df
      |> D.deriveMany
          [ "sepal_ratio"         .= sepal_length / sepal_width
          , "petal_area"          .= petal_width * petal_length
          , "has_wide_sepal"      .= sepal_width .> 3.0
          , "zcore_petal_length"  .= F.zScore petal_length
          , "petal_width_squared" .= F.pow petal_width 2
          ]
```

No imports, main function, or language extensions necessary!

## GHCi
GHCi also gives you a working environment out of the box. 

Run `cabal install dataframe` then `dataframe` (it should be in your path after installation).

Then we can run:

```haskell
dataframe> df <- D.readParquet "./data/iris.parquet"
dataframe> :exposeColumns df
sepal_length :: Expr Double
sepal_width :: Expr Double
petal_length :: Expr Double
petal_width :: Expr Double
variety :: Expr Text
dataframe> df |> D.filterWhere (petal_length .> 6.5)
```