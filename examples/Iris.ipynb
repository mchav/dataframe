{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Iris Classification with Neural Networks in Haskell\n",
    "===================================================\n",
    "\n",
    "This program demonstrates how to build a multiclass classification model\n",
    "in Haskell using the DataFrame and Hasktorch libraries. It's a direct port\n",
    "of the PyTorch tutorial found at:\n",
    "https://machinelearningmastery.com/building-a-multiclass-classification-model-in-pytorch/\n",
    "\n",
    "What This Program Does\n",
    "----------------------\n",
    "\n",
    "1. Loads the famous Iris dataset (flower measurements and species)\n",
    "2. Splits the data into training (70%) and test (30%) sets\n",
    "3. Builds a 3-layer neural network (4 inputs → 8 hidden → 3 outputs)\n",
    "4. Trains the model for 10,000 epochs using gradient descent\n",
    "5. Evaluates performance with confusion matrices and classification metrics\n",
    "\n",
    "Language Extensions and Module Setup\n",
    "-------------------------------------\n",
    "\n",
    "Haskell allows us to enable certain language features using pragmas.\n",
    "Think of these like compiler flags in C++ or decorator syntax in Python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "{-# LANGUAGE DeriveGeneric #-}\n",
    "{-# LANGUAGE MultiParamTypeClasses #-}\n",
    "{-# LANGUAGE OverloadedStrings #-}\n",
    "{-# LANGUAGE RecordWildCards #-}\n",
    "{-# LANGUAGE TypeApplications #-}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Now we import the libraries we need. This is similar to `import` statements\n",
    "in Python or `#include` in C++:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import GHC.Generics (Generic)\n",
    "\n",
    "import Control.Exception (throw)\n",
    "import Control.Monad (when, zipWithM_)\n",
    "import Data.Either\n",
    "import Data.Function (on)\n",
    "import Data.List (maximumBy)\n",
    "\n",
    "import qualified Data.Array as A\n",
    "\n",
    "import qualified Data.Text as T\n",
    "import qualified Data.Vector as V\n",
    "import qualified Data.Vector.Unboxed as VU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "DataFrame is a Haskell library similar to `pandas` in Python:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import DataFrame ((|>))\n",
    "import qualified DataFrame as D\n",
    "import qualified DataFrame.Functions as F\n",
    "import qualified DataFrame.Hasktorch as DHT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Hasktorch is a Haskell binding to `Torch`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import Text.Printf (printf)\n",
    "import qualified Torch as HT\n",
    "import qualified System.Random as SysRand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Defining Our Data Types\n",
    "------------------------\n",
    "\n",
    "In Haskell, we can use the type system to represent our data precisely.\n",
    "The Iris dataset contains three species of flowers, which we represent\n",
    "as an algebraic data type (similar to an enum in other languages):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data Iris\n",
    "    = Setosa\n",
    "    | Versicolor\n",
    "    | Virginica\n",
    "    deriving (Eq, Show, Read, Ord, Enum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "The `deriving` clause automatically generates useful functions:\n",
    "- `Eq`: Allows us to compare Iris values for equality\n",
    "- `Show`: Converts Iris to a String (e.g., \"Setosa\")\n",
    "- `Read`: Converts a String to Iris (e.g., \"Setosa\" → Setosa)\n",
    "- `Ord`: Allows ordering/sorting\n",
    "- `Enum`: Lets us convert to/from integers (Setosa=0, Versicolor=1, Virginica=2)\n",
    "\n",
    "\n",
    "Neural Network Architecture\n",
    "----------------------------\n",
    "\n",
    "We define our Multi-Layer Perceptron (MLP) architecture in two parts:\n",
    "\n",
    "First, a specification that describes the shape of our network:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data MLPSpec = MLPSpec\n",
    "    { inputFeatures :: Int   -- Number of input features (4 for iris)\n",
    "    , hiddenFeatures :: Int  -- Number of neurons in hidden layer\n",
    "    , outputFeatures :: Int  -- Number of output classes (3 species)\n",
    "    }\n",
    "    deriving (Show, Eq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Second, the actual model with its layers. Each layer is a `Linear`\n",
    "transformation (like `nn.Linear` in PyTorch):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data MLP = MLP\n",
    "    { l0 :: HT.Linear  -- Input → Hidden layer\n",
    "    , l1 :: HT.Linear  -- Hidden → Output layer\n",
    "    }\n",
    "    deriving (Generic, Show)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Network Architecture Diagram:\n",
    "\n",
    "    Input Layer (4)  →  Hidden Layer (8)  →  Output Layer (3)\n",
    "    ---------------     -----------------     ----------------\n",
    "    sepal.length        ReLU activation       Softmax\n",
    "    sepal.width         (introduces           (produces\n",
    "    petal.length        non-linearity)        probabilities)\n",
    "    petal.width                               Setosa\n",
    "                                              Versicolor\n",
    "                                              Virginica\n",
    "\n",
    "Making Our Model Trainable\n",
    "---------------------------\n",
    "\n",
    "We need to tell Hasktorch how to initialize our network with random weights.\n",
    "This is similar to defining `__init__()` in a PyTorch `nn.Module`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "instance HT.Parameterized MLP\n",
    "instance HT.Randomizable MLPSpec MLP where\n",
    "    sample MLPSpec{..} =\n",
    "        MLP\n",
    "            <$> HT.sample (HT.LinearSpec inputFeatures hiddenFeatures)\n",
    "            <*> HT.sample (HT.LinearSpec hiddenFeatures outputFeatures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "The `<$>` and `<*>` operators are Haskell's way of working with random\n",
    "initialization. Think of this as: \"Create an MLP by randomly sampling\n",
    "weights for both layers.\"\n",
    "\n",
    "\n",
    "Forward Pass\n",
    "------------\n",
    "\n",
    "This function defines how data flows through the network. It's equivalent\n",
    "to the `forward()` method in PyTorch. Read it from right to left (or\n",
    "bottom to top in the chain):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mlp :: MLP -> HT.Tensor -> HT.Tensor\n",
    "mlp MLP{..} =\n",
    "    HT.softmax (HT.Dim 1)      -- 4. Apply softmax (probabilities sum to 1)\n",
    "        . HT.linear l1          -- 3. Apply second linear layer\n",
    "        . HT.relu               -- 2. Apply ReLU activation\n",
    "        . HT.linear l0          -- 1. Apply first linear layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "In Python/PyTorch, this would look like:\n",
    "```python\n",
    "def forward(self, x):\n",
    "    x = self.l0(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.l1(x)\n",
    "    x = F.softmax(x, dim=1)\n",
    "    return x\n",
    "```\n",
    "\n",
    "\n",
    "Training Loop\n",
    "-------------\n",
    "\n",
    "This is our main training function. It's similar to the epoch loop in\n",
    "PyTorch training code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainLoop ::\n",
    "    Int ->                          -- Number of epochs\n",
    "    (HT.Tensor, HT.Tensor) ->      -- Training features and labels\n",
    "    (HT.Tensor, HT.Tensor) ->      -- Test features and labels\n",
    "    MLP ->                          -- Initial model\n",
    "    IO MLP                          -- Returns trained model\n",
    "trainLoop\n",
    "    n\n",
    "    (features, labels)\n",
    "    (testFeatures, testLabels)\n",
    "    initialM =\n",
    "        HT.foldLoop initialM n $ \\state i -> do\n",
    "            -- Forward pass: compute predictions\n",
    "            let predicted = mlp state features\n",
    "            \n",
    "            -- Compute loss (how wrong our predictions are)\n",
    "            let loss = HT.binaryCrossEntropyLoss' labels predicted\n",
    "            \n",
    "            -- Every 1000 iterations, print progress\n",
    "            when (i `mod` 1000 == 0) $ do\n",
    "                let testPredicted = mlp state testFeatures\n",
    "                let testLoss = HT.binaryCrossEntropyLoss' testLabels testPredicted\n",
    "                putStrLn $\n",
    "                    \"Iteration :\"\n",
    "                        ++ show i\n",
    "                        ++ \" | Training Set Loss: \"\n",
    "                        ++ show (HT.asValue loss :: Float)\n",
    "                        ++ \" | Test Set Loss: \"\n",
    "                        ++ show (HT.asValue testLoss :: Float)\n",
    "            \n",
    "            -- Backward pass: update weights using gradient descent\n",
    "            -- HT.GD is the optimizer, 1e-2 is the learning rate\n",
    "            (state', _) <- HT.runStep state HT.GD loss 1e-2\n",
    "            pure state'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Evaluation Metrics\n",
    "------------------\n",
    "\n",
    "We define a confusion matrix type to track our predictions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type ConfusionMatrix = A.Array (Int, Int) Float\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "The confusion matrix shows actual vs predicted labels:\n",
    "\n",
    "                Predicted\n",
    "              0     1     2\n",
    "    Actual 0  TP    FN    FN\n",
    "           1  FP    TP    FN\n",
    "           2  FP    FP    TP\n",
    "\n",
    "where rows are actual labels and columns are predicted labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "confusionMatrix :: Int -> [Int] -> [Int] -> ConfusionMatrix\n",
    "confusionMatrix n actuals preds = A.accumArray (+) 0 bnds [(x, 1) | x <- zip actuals preds]\n",
    "  where\n",
    "    bnds = ((0, 0), (n - 1, n - 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Helper function to print the confusion matrix in a readable format:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pprintMatrix :: ConfusionMatrix -> String\n",
    "pprintMatrix mtx =\n",
    "    unlines $\n",
    "        header\n",
    "            : [ unwords $ printf \"%5d\" y : [printf \"%5.2f\" (mtx A.! (x, y)) | x <- [x1 .. x2]]\n",
    "              | y <- [y1 .. y2]\n",
    "              ]\n",
    "  where\n",
    "    ((x1, y1), (x2, y2)) = A.bounds mtx\n",
    "    header = unwords $ \"     \" : [printf \"%5d\" x | x <- [x1 .. x2]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Calculate sums for precision and recall:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rowSumAt :: ConfusionMatrix -> Int -> Float\n",
    "rowSumAt mtx n = sum [x | ((i, j), x) <- A.assocs mtx, j == n]\n",
    "\n",
    "colSumAt :: ConfusionMatrix -> Int -> Float\n",
    "colSumAt mtx n = sum [x | ((i, j), x) <- A.assocs mtx, i == n]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Precision = True Positives / (True Positives + False Positives)\n",
    "(Of all the times we predicted class X, how often were we right?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classwisePrecision :: ConfusionMatrix -> [Float]\n",
    "classwisePrecision mtx = [mtx A.! (i, i) / mtx `rowSumAt` i | i <- [y1 .. y2]]\n",
    "  where\n",
    "    ((x1, y1), (x2, y2)) = A.bounds mtx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Recall = True Positives / (True Positives + False Negatives)\n",
    "(Of all the actual class X examples, how many did we find?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classwiseRecall :: ConfusionMatrix -> [Float]\n",
    "classwiseRecall mtx = [mtx A.! (i, i) / mtx `colSumAt` i | i <- [x1 .. x2]]\n",
    "  where\n",
    "    ((x1, y1), (x2, y2)) = A.bounds mtx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Convert one-hot encoded predictions back to class labels.\n",
    "For example: [0.1, 0.8, 0.1] → 1 (because index 1 has highest value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reverseOneHot :: HT.Tensor -> [Int]\n",
    "reverseOneHot tsr = map (fst . maximumBy (compare `on` snd) . zip [0 ..]) vals\n",
    "  where\n",
    "    vals = HT.asValue tsr :: [[Float]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Main Program\n",
    "------------\n",
    "\n",
    "Now we bring it all together!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df <- D.readParquet \"../data/iris.parquet\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "The iris dataset has 5 columns:\n",
    "- sepal.length (Double)\n",
    "- sepal.width (Double)\n",
    "- petal.length (Double)\n",
    "- petal.width (Double)\n",
    "- variety (Text: \"Setosa\", \"Versicolor\", or \"Virginica\")\n",
    "\n",
    "Convert the text labels to integers using our Iris type:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "let derivedDf =\n",
    "        df\n",
    "            |> D.derive\n",
    "                \"variety\"\n",
    "                (F.lift (fromEnum . read @Iris . T.unpack) (F.col \"variety\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "The `|>` operator pipes data left-to-right (like Unix pipes or method chaining).\n",
    "This converts: \"Setosa\" → 0, \"Versicolor\" → 1, \"Virginica\" → 2\n",
    "\n",
    "Step 2: Split into training and test sets\n",
    "==========================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "let (trainDf, testDf) = D.randomSplit (SysRand.mkStdGen 42) 0.7 derivedDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "This is like `train_test_split` in scikit-learn. We use 70% for training\n",
    "and 30% for testing, with a fixed random seed (42) for reproducibility.\n",
    "\n",
    "Step 3: Prepare features (X) and labels (y)\n",
    "============================================\n",
    "\n",
    "Extract the four measurement columns as our features:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "let trainFeaturesTr =\n",
    "        trainDf\n",
    "            |> D.select [\"sepal.length\", \"sepal.width\", \"petal.length\", \"petal.width\"]\n",
    "            |> DHT.toTensor\n",
    "let testFeaturesTr =\n",
    "        testDf\n",
    "            |> D.select [\"sepal.length\", \"sepal.width\", \"petal.length\", \"petal.width\"]\n",
    "            |> DHT.toTensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Extract the labels (species) as integers:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "let trainLabels = either throw id (D.columnAsIntVector \"variety\" trainDf)\n",
    "let testLabels = either throw id (D.columnAsIntVector \"variety\" testDf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Convert labels to one-hot encoding for neural network training:\n",
    "- 0 (Setosa) → [1.0, 0.0, 0.0]\n",
    "- 1 (Versicolor) → [0.0, 1.0, 0.0]\n",
    "- 2 (Virginica) → [0.0, 0.0, 1.0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "let trainLabelsTr = HT.toType HT.Float $ HT.oneHot 3 $ HT.asTensor $ trainLabels\n",
    "let testLabelsTr = HT.toType HT.Float $ HT.oneHot 3 $ HT.asTensor $ testLabels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Step 4: Initialize the neural network\n",
    "======================================\n",
    "\n",
    "Create a random initial model with:\n",
    "- 4 input neurons (one for each feature)\n",
    "- 8 hidden neurons (arbitrary choice, can be tuned)\n",
    "- 3 output neurons (one for each species)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initialModel <- HT.sample $ MLPSpec 4 8 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Step 5: Train the model\n",
    "========================\n",
    "\n",
    "Run 10,000 training iterations (epochs):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainedModel <-\n",
    "    trainLoop\n",
    "        10000\n",
    "        (trainFeaturesTr, trainLabelsTr)\n",
    "        (testFeaturesTr, testLabelsTr)\n",
    "        initialModel\n",
    "\n",
    "putStrLn \"Your model weights are given as follows: \"\n",
    "print trainedModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Step 6: Evaluate on training set\n",
    "=================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "putStrLn \".....................................\"\n",
    "putStrLn \".....................................\"\n",
    "putStrLn \"Training Set Summary is as follows: \"\n",
    "\n",
    "let predTrain = reverseOneHot $ mlp trainedModel trainFeaturesTr\n",
    "putStrLn \"====== Confusion Matrix ========\"\n",
    "let confusionTrain = confusionMatrix 3 (VU.toList trainLabels) predTrain\n",
    "putStrLn $ pprintMatrix confusionTrain\n",
    "\n",
    "putStrLn \"=========== Classwise Metrics =============\"\n",
    "print $ D.fromNamedColumns\n",
    "    [ (\"variety\" , D.fromList (map (toEnum @Iris) [0 .. 2]))\n",
    "    , (\"precision\", D.fromList (classwisePrecision confusionTrain))\n",
    "    , (\"recall\", D.fromList (classwiseRecall confusionTrain))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Step 7: Evaluate on test set\n",
    "=============================\n",
    "\n",
    "This is the true test of our model - how well does it perform on data\n",
    "it has never seen before?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "putStrLn \".....................................\"\n",
    "putStrLn \".....................................\"\n",
    "putStrLn \"Test Set Summary is as follows: \"\n",
    "\n",
    "let predTest = reverseOneHot $ mlp trainedModel testFeaturesTr\n",
    "putStrLn \"====== Confusion Matrix ========\"\n",
    "let confusionTest = confusionMatrix 3 (VU.toList testLabels) predTest\n",
    "putStrLn $ pprintMatrix confusionTest\n",
    "\n",
    "putStrLn \"=========== Classwise Metrics =============\"\n",
    "print $ D.fromNamedColumns\n",
    "    [ (\"variety\" , D.fromList (map (toEnum @Iris) [0 .. 2]))\n",
    "    , (\"precision\", D.fromList (classwisePrecision confusionTest))\n",
    "    , (\"recall\", D.fromList (classwiseRecall confusionTest))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": false
   },
   "source": [
    "Conclusion\n",
    "==========\n",
    "\n",
    "This program demonstrates that Haskell can be used for machine learning\n",
    "tasks just like Python! The strong type system helps catch errors at\n",
    "compile time, and the functional style leads to concise, composable code.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Haskell",
   "language": "haskell",
   "name": "haskell"
  },
  "language_info": {
   "name": "haskell",
   "version": "9.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
